{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a43374",
   "metadata": {},
   "source": [
    "## Clustering Basado en las acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ce03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = \"https://raw.githubusercontent.com/Andru-1987/74235-_DataScience_I/refs/heads/main/clase_6/storage/acciones/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_map = {\n",
    "    \"D\": \"Dominion Energy Inc.\",\n",
    "    \"EXC\": \"Exelon Corp.\",\n",
    "    \"NEE\": \"NextEra Energy Inc.\",\n",
    "    \"SO\": \"Southern Co.\",\n",
    "    \"DUK\": \"Duke Energy Corp.\"\n",
    "}\n",
    "\n",
    "matched_file_names = {\n",
    "    symbol: url_base + symbol + \".csv\" for symbol in symbols_map.keys()\n",
    "}\n",
    "\n",
    "final_dataframe = []\n",
    "\n",
    "for symbol_name, path_csv in matched_file_names.items():\n",
    "    df = pd.read_csv(path_csv, sep=\",\")\n",
    "    df[\"Name\"] = symbols_map[symbol_name]\n",
    "    final_dataframe.append(df)\n",
    "\n",
    "final_dataframe = pd.concat(final_dataframe, ignore_index=True)\n",
    "final_dataframe.columns = final_dataframe.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa52c9d",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e490d282",
   "metadata": {},
   "source": [
    "# Análisis de Clustering para Acciones del Mercado\n",
    "\n",
    "## Preparación de Datos\n",
    "\n",
    "Para identificar acciones similares, necesitamos extraer características que capturen el comportamiento de cada acción:\n",
    "\n",
    "1. **Precio promedio**: Nivel general de precio\n",
    "2. **Volatilidad**: Rango entre máximo y mínimo\n",
    "3. **Cambio diario**: Diferencia entre cierre y apertura\n",
    "4. **Volumen promedio**: Actividad de trading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a832722",
   "metadata": {},
   "source": [
    "\n",
    "## Algoritmos de Clustering Aplicables\n",
    "\n",
    "### 1. K-means (No Jerárquico)\n",
    "\n",
    "**Ventajas para este caso:**\n",
    "- Simple y eficiente\n",
    "- Funciona bien con datos numéricos continuos como precios\n",
    "- Útil cuando conocemos aproximadamente cuántas empresas similares esperamos\n",
    "\n",
    "**Desventajas:**\n",
    "- Requiere definir el número de clusters previamente\n",
    "- Sensible a outliers (valores extremos)\n",
    "\n",
    "**Cuándo usarlo:** Si queremos agrupar las 5 empresas en 2-3 categorías conocidas (ej: alto, medio, bajo rendimiento).\n",
    "\n",
    "### 2. Clustering Jerárquico Aglomerativo\n",
    "\n",
    "**Ventajas para este caso:**\n",
    "- No requiere definir número de clusters anticipadamente\n",
    "- Genera un dendrograma que muestra relaciones entre todas las acciones\n",
    "- Útil para explorar similitudes entre empresas\n",
    "\n",
    "**Desventajas:**\n",
    "- Más costoso computacionalmente\n",
    "- Menos eficiente con datasets grandes\n",
    "\n",
    "**Cuándo usarlo:** Para análisis exploratorio y visualizar qué acciones son más parecidas entre sí.\n",
    "\n",
    "### 3. DBSCAN (Densidad)\n",
    "\n",
    "**Ventajas para este caso:**\n",
    "- Identifica clusters de forma arbitraria\n",
    "- Detecta outliers automáticamente\n",
    "\n",
    "**Desventajas:**\n",
    "- Requiere ajustar parámetros (epsilon, min_samples)\n",
    "- Con solo 5 empresas, puede no ser la mejor opción\n",
    "- Necesita suficiente densidad de datos\n",
    "\n",
    "**Cuándo usarlo:** Si tuviéramos muchas más acciones y quisiéramos identificar grupos naturales sin asumir formas esféricas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c6c60",
   "metadata": {},
   "source": [
    "\n",
    "## Recomendación para este Caso\n",
    "\n",
    "**Algoritmo sugerido: Clustering Jerárquico Aglomerativo**\n",
    "\n",
    "**Justificación:**\n",
    "1. Tenemos pocas empresas (5), lo que hace manejable el costo computacional\n",
    "2. No sabemos a priori cuántos grupos naturales existen\n",
    "3. El dendrograma nos permitirá visualizar claramente las similitudes\n",
    "4. Es más interpretable para análisis inicial\n",
    "\n",
    "**Alternativa válida:** K-means con k=2 o k=3 si queremos clasificar las acciones en categorías predefinidas (ej: crecimiento alto vs. estable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85afc70f",
   "metadata": {},
   "source": [
    "\n",
    "## Métricas de Evaluación\n",
    "\n",
    "### Métricas Internas\n",
    "\n",
    "Estas métricas evalúan la calidad del clustering sin información externa:\n",
    "\n",
    "#### 1. Coeficiente de Silueta (Silhouette Score)\n",
    "- **Rango:** -1 a 1\n",
    "- **Interpretación:**\n",
    "  - Cerca de 1: Puntos bien agrupados en su cluster\n",
    "  - Cerca de 0: Puntos en el límite entre clusters\n",
    "  - Negativo: Probablemente mal asignados\n",
    "- **Valor ideal:** > 0.5\n",
    "\n",
    "#### 2. Índice de Davies-Bouldin\n",
    "- **Rango:** 0 a infinito\n",
    "- **Interpretación:** Mide la relación entre dispersión intra-cluster y separación inter-cluster\n",
    "- **Valor ideal:** Más bajo es mejor (clusters más compactos y separados)\n",
    "\n",
    "#### 3. Índice de Calinski-Harabasz\n",
    "- **Rango:** 0 a infinito\n",
    "- **Interpretación:** Ratio de dispersión entre clusters vs. dentro de clusters\n",
    "- **Valor ideal:** Más alto es mejor\n",
    "\n",
    "### Métricas para Jerárquico\n",
    "\n",
    "#### 4. Coeficiente de Correlación Cofenética\n",
    "- **Rango:** -1 a 1\n",
    "- **Interpretación:** Qué tan bien el dendrograma preserva las distancias originales\n",
    "- **Valor ideal:** > 0.7 indica buena representación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d295acce",
   "metadata": {},
   "source": [
    "\n",
    "### Cómo Interpretar Resultados\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "Si obtenemos:\n",
    "- Silhouette Score = 0.65\n",
    "- Davies-Bouldin = 0.8\n",
    "- Calinski-Harabasz = 150\n",
    "\n",
    "**Interpretación:** Los clusters están bien definidos (Silhouette alto), son compactos y separados (Davies-Bouldin bajo), con buena separación general (Calinski alto).\n",
    "\n",
    "### Validación del Número de Clusters\n",
    "\n",
    "**Método del Codo:**\n",
    "- Graficar inercia (suma de distancias cuadradas) vs. número de clusters\n",
    "- Buscar el \"codo\" donde la mejora se reduce significativamente\n",
    "\n",
    "**Análisis de Silueta:**\n",
    "- Calcular Silhouette Score para diferentes valores de k\n",
    "- Elegir k que maximice este valor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51ab15",
   "metadata": {},
   "source": [
    "\n",
    "## Flujo de Trabajo Recomendado\n",
    "\n",
    "1. **Extraer características** de cada acción (medias, volatilidades, tendencias)\n",
    "2. **Normalizar datos** (importante: las variables tienen escalas diferentes)\n",
    "3. **Aplicar clustering jerárquico** con diferentes métodos de enlace (ward, complete, average)\n",
    "4. **Visualizar dendrograma** para identificar número natural de clusters\n",
    "5. **Calcular métricas** (Silhouette, Davies-Bouldin, Calinski-Harabasz)\n",
    "6. **Validar con K-means** usando el número de clusters identificado\n",
    "7. **Interpretar resultados** en contexto del negocio\n",
    "\n",
    "## Consideraciones Finales\n",
    "\n",
    "- La normalización es crítica porque precios y volúmenes tienen magnitudes muy diferentes\n",
    "- Considerar características temporales (tendencias, estacionalidad)\n",
    "- Validar resultados con conocimiento del dominio (sector energético)\n",
    "- Las métricas son guías, no verdades absolutas: la interpretabilidad importa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cdf546",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24752acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9babf106",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>Apartado especial sobre por que el StandarScaler y no el Normalizer</strong></summary>\n",
    "\n",
    "## Diferencias entre StandardScaler y Normalizer\n",
    "\n",
    "### StandardScaler (Z-score normalization)\n",
    "```python\n",
    "# Fórmula: (x - media) / desviación_estándar\n",
    "# Resultado: media = 0, desviación estándar = 1\n",
    "```\n",
    "\n",
    "**Transforma por columna (feature)**: Cada característica queda centrada en 0\n",
    "\n",
    "### Normalizer\n",
    "```python\n",
    "# Fórmula: x / ||x|| (norma del vector)\n",
    "# Resultado: cada fila tiene norma = 1\n",
    "```\n",
    "\n",
    "**Transforma por fila (muestra)**: Cada empresa queda con vector unitario\n",
    "\n",
    "## Por qué StandardScaler es mejor aquí:\n",
    "\n",
    "### 1. **Diferentes escalas entre features**\n",
    "```\n",
    "- Precio promedio: $30 - $140\n",
    "- Volumen promedio: 1,500,000 - 10,000,000\n",
    "- Volatilidad: $0.5 - $2.5\n",
    "- Cambio diario: $-0.5 - $1.0\n",
    "```\n",
    "\n",
    "StandardScaler **equipara la importancia** de todas las variables. Sin esto, el volumen dominaría el clustering por ser mucho mayor numéricamente.\n",
    "\n",
    "### 2. **Interpretabilidad**\n",
    "Con StandardScaler:\n",
    "- Valores positivos = por encima del promedio del mercado\n",
    "- Valores negativos = por debajo del promedio\n",
    "- La magnitud indica cuántas desviaciones estándar está del promedio\n",
    "\n",
    "### 3. **Algoritmos basados en distancia**\n",
    "K-Means y clustering jerárquico calculan distancias euclidianas. StandardScaler asegura que cada feature contribuya equitativamente a esa distancia.\n",
    "\n",
    "## Cuándo usar Normalizer:\n",
    "\n",
    "Normalizer sería útil si estuviéramos interesados en **la dirección del comportamiento**, no la magnitud. Por ejemplo:\n",
    "- Análisis de texto (vectores de palabras)\n",
    "- Cuando la escala absoluta no importa, solo las proporciones\n",
    "\n",
    "## Ejemplo práctico con tu dataset:\n",
    "\n",
    "```python\n",
    "# CON STANDARDSCALER (CORRECTO)\n",
    "Southern Co.:     [precio: -0.5, volumen: 0.3, ...]\n",
    "Dominion Energy:  [precio: 0.2,  volumen: -0.4, ...]\n",
    "# Las features están balanceadas\n",
    "\n",
    "# CON NORMALIZER (INCORRECTO PARA ESTE CASO)\n",
    "Southern Co.:     [precio: 0.001, volumen: 0.999, ...]\n",
    "Dominion Energy:  [precio: 0.002, volumen: 0.998, ...]\n",
    "# El volumen domina completamente por ser más grande\n",
    "```\n",
    "\n",
    "## Recomendación final:\n",
    "\n",
    "Para análisis de clustering de acciones basado en características financieras con diferentes unidades y escalas, **siempre usa StandardScaler**.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac93e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_dataframe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular características por empresa\n",
    "features = df.groupby('name').agg({\n",
    "    'close': 'mean',\n",
    "    'volume': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "# Agregar volatilidad y cambio\n",
    "df['volatility'] = df['high'] - df['low']\n",
    "df['change'] = df['close'] - df['open']\n",
    "\n",
    "volatility_change = df.groupby('name').agg({\n",
    "    'volatility': 'mean',\n",
    "    'change': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "features = features.merge(volatility_change, on='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89799d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a767d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(features[['close', 'volume', 'volatility', 'change']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc162ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame con datos normalizados\n",
    "features_normalized = pd.DataFrame(\n",
    "    X,\n",
    "    columns=['close_norm', 'volume_norm', 'volatility_norm', 'change_norm']\n",
    ")\n",
    "features_normalized['name'] = features['name'].values\n",
    "features_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e259e5",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "features_normalized['cluster_kmeans'] = kmeans.fit_predict(X)\n",
    "features_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0d480",
   "metadata": {},
   "source": [
    "- ¿Por qué es adecuado?\n",
    "\n",
    "    * Funciona bien si los datos tienen grupos esféricos\n",
    "    * Rápido y eficiente para cientos de acciones\n",
    "    * Nos permite elegir cuántos grupos queremos\n",
    "\n",
    "- Limitación:\n",
    "\n",
    "    * Requiere definir k\n",
    "    * Sensible a valores extremos\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f819c9f",
   "metadata": {},
   "source": [
    "## Clustering Aglomerativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ecc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agglo = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "features_normalized['cluster_agglo'] = agglo.fit_predict(X)\n",
    "features_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba22e4",
   "metadata": {},
   "source": [
    "- ¿Por qué podría funcionar?\n",
    "\n",
    "    - No requiere inicialización ni supuestos fuertes\n",
    "    - Útil para ver dendrogramas y relaciones entre acciones\n",
    "\n",
    "- Limitación:\n",
    "\n",
    "    - Escalabilidad (malo con miles de acciones)\n",
    "    - Menos robusto con ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf9506e",
   "metadata": {},
   "source": [
    "## Clustering DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=1.2, min_samples=2)\n",
    "features_normalized['cluster_dbscan'] = dbscan.fit_predict(X)\n",
    "features_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cd36d0",
   "metadata": {},
   "source": [
    "- ¿Por qué se usa?\n",
    "\n",
    "    - Identifica outliers\n",
    "    - Detecta clusters de formas arbitrarias\n",
    "\n",
    "- ¿Por qué no es ideal aquí?\n",
    "\n",
    "    - Esperamos que todas las acciones estén en clusters\n",
    "    - No tenemos ruido, ni distribución espacial clara\n",
    "    - Difícil de configurar los parámetros eps y min_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349a208",
   "metadata": {},
   "source": [
    "## Muestra de clusterizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d6cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    pca = PCA(n_components=2, random_state=42, svd_solver='auto')\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    print(\"PCA exitoso\")\n",
    "except Exception as e:\n",
    "    print(f\"Error en PCA: {e}\")\n",
    "    # Alternativa: usar componentes principales manualmente\n",
    "    from scipy.linalg import svd\n",
    "    U, s, Vt = svd(X, full_matrices=False)\n",
    "    X_pca = U[:, :2] * s[:2]\n",
    "\n",
    "# DataFrame para graficar\n",
    "df_plot = pd.DataFrame(X_pca, columns=[\"PCA1\", \"PCA2\"])\n",
    "df_plot['cluster'] = features_normalized['cluster_kmeans']\n",
    "df_plot['name'] = features_normalized['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ecb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_ellipse(position, covariance, ax, color, alpha=0.2):\n",
    "    try:\n",
    "        # Verificar que la matriz de covarianza sea válida\n",
    "        if covariance.shape == (2, 2) and not np.isnan(covariance).any():\n",
    "            # Usar SVD con manejo de errores\n",
    "            U, s, Vt = np.linalg.svd(covariance)\n",
    "            angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "            width, height = 2 * np.sqrt(s)\n",
    "            \n",
    "            # Limitar el tamaño máximo de las elipses\n",
    "            width = min(width, 3)\n",
    "            height = min(height, 3)\n",
    "            \n",
    "            ellipse = Ellipse(position, width, height, angle=angle, \n",
    "                            color=color, alpha=alpha)\n",
    "            ax.add_patch(ellipse)\n",
    "    except Exception as e:\n",
    "        print(f\"Advertencia: No se pudo dibujar elipse: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a5ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paleta de colores\n",
    "palette = sns.color_palette(\"Set1\", n_colors=len(df_plot['cluster'].unique()))\n",
    "cluster_colors = {cluster: palette[i] for i, cluster in enumerate(sorted(df_plot['cluster'].unique()))}\n",
    "\n",
    "# VISUALIZACIÓN MEJORADA DE CLUSTERS\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Gráfico 1: Clustering con PCA y nombres completos\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "\n",
    "# Scatter plot con colores por cluster\n",
    "scatter = sns.scatterplot(\n",
    "    data=df_plot,\n",
    "    x=\"PCA1\", y=\"PCA2\",\n",
    "    hue=\"cluster\",\n",
    "    palette=cluster_colors,\n",
    "    s=120,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1,\n",
    "    ax=ax1\n",
    ")\n",
    "\n",
    "# Etiquetas con nombres completos de las acciones\n",
    "for i, row in df_plot.iterrows():\n",
    "    ax1.annotate(df_plot['name'][i], \n",
    "                (row['PCA1'], row['PCA2']),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontsize=9,\n",
    "                alpha=0.8,\n",
    "                bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.7))\n",
    "\n",
    "# Dibujar elipses para cada cluster\n",
    "for cluster_id in df_plot['cluster'].unique():\n",
    "    cluster_points = df_plot[df_plot['cluster'] == cluster_id][[\"PCA1\", \"PCA2\"]]\n",
    "    if len(cluster_points) > 1:\n",
    "        center = cluster_points.mean().values\n",
    "        cov = np.cov(cluster_points.T)\n",
    "        color = cluster_colors[cluster_id]\n",
    "        draw_ellipse(center, cov, ax1, color=color)\n",
    "\n",
    "ax1.set_title(\"Clustering de Acciones - Visualización PCA\", fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel(f\"Componente Principal 1 ({pca.explained_variance_ratio_[0]:.1%} varianza)\")\n",
    "ax1.set_ylabel(f\"Componente Principal 2 ({pca.explained_variance_ratio_[1]:.1%} varianza)\")\n",
    "ax1.legend(title=\"Cluster\", loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b231bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLA RESUMEN POR CLUSTER\n",
    "print(\"\\n=== RESUMEN POR CLUSTER ===\")\n",
    "for cluster_id in sorted(features_normalized['cluster_kmeans'].unique()):\n",
    "    cluster_data = features_normalized[features_normalized['cluster_kmeans'] == cluster_id]\n",
    "    print(f\"\\n--- CLUSTER {cluster_id} ---\")\n",
    "    print(f\"Acciones incluidas ({len(cluster_data)}):\")\n",
    "    for name in cluster_data['name'].unique():\n",
    "        count = len(cluster_data[cluster_data['name'] == name])\n",
    "        print(f\"  - {name}: {count} acción(es)\")\n",
    "    \n",
    "    print(f\"Precio promedio de cierre: ${cluster_data['close_norm'].mean():.2f}\")\n",
    "    print(f\"Volumen promedio: {cluster_data['volume_norm'].mean():,.0f}\")\n",
    "    print(f\"Volatilidad promedio: ${cluster_data['volatility_norm'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRÁFICO ADICIONAL: Heatmap de correlación entre clusters\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calcular características promedio por cluster\n",
    "columns = [\n",
    "\"close_norm\", \"volume_norm\",\t\"volatility_norm\",\t\"change_norm\"]\n",
    "\n",
    "cluster_means = features_normalized.groupby('cluster_kmeans')[columns].mean()\n",
    "\n",
    "# Heatmap de similitud entre clusters\n",
    "similarity_matrix = np.corrcoef(cluster_means.T)\n",
    "\n",
    "sns.heatmap(similarity_matrix, \n",
    "           xticklabels=['open', 'high', 'low', 'close', 'volume'],\n",
    "           yticklabels=['open', 'high', 'low', 'close', 'volume'],\n",
    "           annot=True, cmap='coolwarm', center=0,\n",
    "           square=True, fmt='.2f')\n",
    "plt.title('Matriz de Correlación entre Características\\n(Similitud entre Clusters)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "77695-data-science-i-flex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
