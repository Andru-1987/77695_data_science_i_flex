{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2803e73e",
   "metadata": {},
   "source": [
    "## Regresi√≥n lineal supervisada:\n",
    "\n",
    "### 1. **Regresi√≥n Lineal Simple**\n",
    "\n",
    "* **Qu√© es**: Relaciona una sola variable independiente $X$ con la variable dependiente $y$.\n",
    "  Ejemplo: predecir el precio de una casa usando √∫nicamente los metros cuadrados.\n",
    "* **Cu√°ndo usarla**:\n",
    "\n",
    "  * Si tienes **una √∫nica feature**.\n",
    "  * Cuando quieras interpretar f√°cilmente la pendiente e intercepto.\n",
    "* **Afectaci√≥n por outliers**: Muy sensible üö®, porque un valor extremo puede ‚Äúinclinar‚Äù la recta hacia √©l.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Regresi√≥n Lineal M√∫ltiple**\n",
    "\n",
    "* **Qu√© es**: Usa varias variables independientes para explicar el target.\n",
    "  Ejemplo: predecir el precio de una casa usando metros cuadrados, n√∫mero de habitaciones, ubicaci√≥n, etc.\n",
    "* **Cu√°ndo usarla**:\n",
    "\n",
    "  * Cuando el fen√≥meno depende de **varios factores**.\n",
    "  * Si quieres un modelo m√°s explicativo y no solo predictivo.\n",
    "* **Afectaci√≥n por outliers**: Tambi√©n sensible üö®, ya que los valores extremos en alguna variable pueden distorsionar los coeficientes.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Regresi√≥n Ridge (L2 Regularization)**\n",
    "\n",
    "* **Qu√© es**: Variante de la lineal m√∫ltiple con una penalizaci√≥n al tama√±o de los coeficientes ($\\lambda \\sum \\beta^2$).\n",
    "* **Cu√°ndo usarla**:\n",
    "\n",
    "  * Si tienes **muchas variables** y riesgo de *multicolinealidad*.\n",
    "  * Cuando quieres evitar sobreajuste.\n",
    "* **Afectaci√≥n por outliers**: Sigue siendo sensible üö®, aunque la regularizaci√≥n reduce un poco el impacto.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Regresi√≥n Lasso (L1 Regularization)**\n",
    "\n",
    "* **Qu√© es**: Similar a Ridge, pero la penalizaci√≥n es la suma de los valores absolutos ($\\lambda \\sum |\\beta|$). Esto hace que algunos coeficientes puedan ser exactamente **0** ‚Üí selecciona variables.\n",
    "* **Cu√°ndo usarla**:\n",
    "\n",
    "  * Cuando quieres hacer **selecci√≥n de features**.\n",
    "  * √ötil si sospechas que solo algunas variables son realmente importantes.\n",
    "* **Afectaci√≥n por outliers**: A√∫n sensible, pero algo m√°s robusta que la lineal simple/m√∫ltiple.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Regresi√≥n Elastic Net**\n",
    "\n",
    "* **Qu√© es**: Combinaci√≥n de Lasso y Ridge.\n",
    "* **Cu√°ndo usarla**:\n",
    "\n",
    "  * Cuando tienes **muchas variables correlacionadas** y adem√°s quieres **selecci√≥n de features**.\n",
    "  * M√°s flexible que usar solo Ridge o solo Lasso.\n",
    "* **Afectaci√≥n por outliers**: Sigue siendo sensible, aunque la regularizaci√≥n ayuda a mitigar un poco el efecto.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Regresi√≥n Robusta**\n",
    "\n",
    "* **Qu√© es**: M√©todos como **RANSAC**, **Huber Regressor**, **Theil-Sen**. Est√°n dise√±ados para ser menos sensibles a los outliers.\n",
    "* **Cu√°ndo usarla**:\n",
    "\n",
    "  * Cuando sabes que en tus datos hay outliers inevitables (errores de medici√≥n, datos muy ruidosos).\n",
    "  * Cuando los m√©todos lineales cl√°sicos ‚Äúse tuercen‚Äù mucho con pocos valores extremos.\n",
    "* **Afectaci√≥n por outliers**: Mucho m√°s **robusta ‚úÖ**, ignoran o reducen el peso de los valores extremos.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Resumen visual (qu√© usar y sensibilidad a outliers)\n",
    "\n",
    "| Modelo                  | Cu√°ndo usarlo                                       | Sensibilidad a outliers      |\n",
    "| ----------------------- | --------------------------------------------------- | ---------------------------- |\n",
    "| Lineal simple           | 1 variable                                          | üö® Alta                      |\n",
    "| Lineal m√∫ltiple         | Varias variables                                    | üö® Alta                      |\n",
    "| Ridge                   | Evitar sobreajuste, multicolinealidad               | üö® Alta                      |\n",
    "| Lasso                   | Selecci√≥n de variables                              | üö® Alta (pero algo mitigada) |\n",
    "| Elastic Net             | Mix Ridge + Lasso, muchas variables correlacionadas | üö® Alta                      |\n",
    "| Robusta (RANSAC, Huber) | Datos con outliers                                  | ‚úÖ Baja                       |\n",
    "\n",
    "---\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "* Si tu dataset est√° **limpio, sin outliers fuertes** ‚Üí usa lineal simple/m√∫ltiple o con regularizaci√≥n (Ridge, Lasso, Elastic Net).\n",
    "* Si tu dataset **tiene outliers inevitables** ‚Üí considera **RANSAC, Huber o Theil-Sen**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1738ae52",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ce971",
   "metadata": {},
   "source": [
    "## Vamos a hacer un ejemplo de regresion simple con LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b099825",
   "metadata": {},
   "source": [
    "# An√°lisis de Autos Usados\n",
    "\n",
    "## ¬øQu√© es Lasso?\n",
    "\n",
    "**Lasso (Least Absolute Shrinkage and Selection Operator)** es una t√©cnica de regularizaci√≥n utilizada en modelos de regresi√≥n lineal. Su principal objetivo es **prevenir el sobreajuste** (*overfitting*) y **mejorar la interpretabilidad** del modelo, ya que puede reducir algunos coeficientes exactamente a cero. Esto permite que el modelo use solo las variables m√°s relevantes.\n",
    "\n",
    "---\n",
    "\n",
    "## ¬øEn qu√© consiste?\n",
    "\n",
    "Lasso agrega una penalizaci√≥n basada en la suma de los valores absolutos de los coeficientes al t√©rmino de p√©rdida del modelo.\n",
    "\n",
    "La funci√≥n de costo se expresa de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\text{Costo}_{Lasso} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p}|\\beta_j|\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "* $y_i$: valor real.\n",
    "* $\\hat{y}_i$: valor predicho.\n",
    "* $\\beta_j$: coeficientes del modelo.\n",
    "* $\\lambda$: par√°metro de regularizaci√≥n (controla la penalizaci√≥n).\n",
    "* $n$: n√∫mero de muestras.\n",
    "* $p$: n√∫mero de caracter√≠sticas (features).\n",
    "\n",
    "El par√°metro $\\lambda$ es clave:\n",
    "\n",
    "* Si $\\lambda = 0$, el modelo se comporta como una regresi√≥n lineal normal.\n",
    "* Si $\\lambda$ aumenta, m√°s coeficientes tienden a reducirse a cero.\n",
    "\n",
    "---\n",
    "\n",
    "## ¬øQu√© es la ‚Äúmatriz de Lasso‚Äù?\n",
    "\n",
    "En sentido estricto, no existe una matriz especial llamada ‚Äúmatriz de Lasso‚Äù.\n",
    "Lo que s√≠ se utiliza en el entrenamiento es:\n",
    "\n",
    "1. **La matriz de caracter√≠sticas (X):** contiene los datos de entrada, de tama√±o $(n \\times p)$.\n",
    "2. **El vector de salida (y):** contiene los valores que se quieren predecir.\n",
    "3. **El modelo Lasso:** que encuentra los coeficientes $\\beta$ resolviendo un problema de optimizaci√≥n regularizado.\n",
    "\n",
    "Por lo tanto, cuando se habla de la ‚Äúmatriz de Lasso‚Äù, normalmente se hace referencia de forma informal a la **matriz de dise√±o (X)**, que es la base para ajustar el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "## Ventajas de Lasso\n",
    "\n",
    "* Realiza **selecci√≥n autom√°tica de variables**, eliminando aquellas menos relevantes.\n",
    "* Produce modelos m√°s simples y f√°ciles de interpretar.\n",
    "* Ayuda a **controlar el sobreajuste**, especialmente cuando hay muchas variables o cuando algunas est√°n muy correlacionadas.\n",
    "\n",
    "---\n",
    "\n",
    "## Cu√°ndo usar Lasso\n",
    "\n",
    "* Cuando se sospecha que muchas variables no aportan informaci√≥n √∫til.\n",
    "* Cuando se busca un modelo con menos variables y m√°s f√°cil de interpretar.\n",
    "* En casos con **m√°s caracter√≠sticas que observaciones**, o cuando existe **multicolinealidad** entre las variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"./datasets/car_data.csv\")\n",
    "df_raw = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf51d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4174a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Fuel_Type.value_counts())\n",
    "print(df.Seller_Type.value_counts())\n",
    "print(df.Transmission.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4eeddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding \"Fuel_Type\" Column\n",
    "df.replace({'Fuel_Type':{'Petrol':0,'Diesel':1,'CNG':2}},inplace=True)\n",
    "\n",
    "# encoding \"Seller_Type\" Column\n",
    "df.replace({'Seller_Type':{'Dealer':0,'Individual':1}},inplace=True)\n",
    "\n",
    "# encoding \"Transmission\" Column\n",
    "df.replace({'Transmission':{'Manual':0,'Automatic':1}},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf72bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Car_Name','Selling_Price'],axis=1)\n",
    "Y = df['Selling_Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6870935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04369a64",
   "metadata": {},
   "source": [
    "stats OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_intercepto = sm.add_constant(X_train)  # Intercepto + variables num√©ricas\n",
    "model_sm = sm.OLS(Y_train.astype(float), X_intercepto).fit()  # Asegurar que 'y' tambi√©n sea float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438d8c43",
   "metadata": {},
   "source": [
    "## Trainning -> Regression Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65812d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a98b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_prediction = lin_reg_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4462ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_score = metrics.r2_score(Y_train, training_data_prediction)\n",
    "print(\"R squared Error : \", error_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_train, training_data_prediction)\n",
    "plt.xlabel(\"Precio Real\")\n",
    "plt.ylabel(\"Precio Predicho\")\n",
    "plt.title(\"Precios Reales vs Precios Predichos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03abae82",
   "metadata": {},
   "source": [
    "## Testing -> Regresion Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b866b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_prediction = lin_reg_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c76b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_score = metrics.r2_score(Y_test, test_data_prediction)\n",
    "print(\"R squared Error : \", error_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0985ab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_test, test_data_prediction)\n",
    "plt.xlabel(\"Precio Real\")\n",
    "plt.ylabel(\"Precio Predicho\")\n",
    "plt.title(\"Precios Reales vs Precios Predichos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7152d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy import stats\n",
    "\n",
    "def regression_summary(model, X, y, feature_names=None):\n",
    "    \"\"\"\n",
    "    Genera un resumen tipo statsmodels para modelos lineales de scikit-learn.\n",
    "    \"\"\"\n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # N√∫mero de observaciones y variables\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    # Error residual\n",
    "    residuals = y - y_pred\n",
    "    RSS = np.sum(residuals ** 2)\n",
    "    MSE = RSS / (n - p - 1)\n",
    "    \n",
    "    # Matriz de dise√±o (agregamos intercepto)\n",
    "    X_design = np.column_stack([np.ones(n), X])\n",
    "    \n",
    "    # Varianza de los coeficientes\n",
    "    var_beta = MSE * np.linalg.inv(X_design.T @ X_design).diagonal()\n",
    "    std_err = np.sqrt(var_beta)\n",
    "    \n",
    "    # Coeficientes (incluyendo intercepto)\n",
    "    coef = np.insert(model.coef_, 0, model.intercept_)\n",
    "    \n",
    "    # Estad√≠stico t y p-values\n",
    "    t_stats = coef / std_err\n",
    "    p_values = [2 * (1 - stats.t.cdf(np.abs(t), df=n - p - 1)) for t in t_stats]\n",
    "    \n",
    "    # Intervalos de confianza\n",
    "    conf_int_low = coef - 1.96 * std_err\n",
    "    conf_int_high = coef + 1.96 * std_err\n",
    "    \n",
    "    # Nombres de las variables\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"x{i}\" for i in range(1, p + 1)]\n",
    "    feature_names = [\"Intercept\"] + feature_names\n",
    "    \n",
    "    # Construir DataFrame resumen\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Coef\": coef,\n",
    "        \"Std Err\": std_err,\n",
    "        \"t\": t_stats,\n",
    "        \"P>|t|\": p_values,\n",
    "        \"[0.025\": conf_int_low,\n",
    "        \"0.975]\": conf_int_high\n",
    "    }, index=feature_names)\n",
    "    \n",
    "    # R¬≤ y m√©tricas globales\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    \n",
    "    print(\"\\nResumen de Regresi√≥n\")\n",
    "    print(\"============================================\")\n",
    "    print(f\"Observaciones: {n}\")\n",
    "    print(f\"Variables: {p}\")\n",
    "    print(f\"R¬≤: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(summary_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_summary(lin_reg_model, X_train, Y_train, feature_names=X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0961d0b4",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lass_reg_model = Lasso()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "lass_reg_model.fit(X_train,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e714f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_prediction = lass_reg_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_score = metrics.r2_score(Y_train, training_data_prediction)\n",
    "print(\"R squared Error : \", error_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_train, training_data_prediction)\n",
    "plt.xlabel(\"Precio Real\")\n",
    "plt.ylabel(\"Precio Predicho\")\n",
    "plt.title(\"Precios Reales vs Precios Predichos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1180d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_prediction = lass_reg_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e4d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "error_score = metrics.r2_score(Y_test, test_data_prediction)\n",
    "print(\"R squared Error : \", error_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c53a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_test, test_data_prediction)\n",
    "plt.xlabel(\"Precio Real\")\n",
    "plt.ylabel(\"Precio Predicho\")\n",
    "plt.title(\"Precios Reales vs Precios Predichos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f85c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_summary(lass_reg_model, X_train, Y_train, feature_names=X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2b34e",
   "metadata": {},
   "source": [
    "##  **Modelo 1: Regresi√≥n Lineal**\n",
    "\n",
    "* **R¬≤ = 0.8799** ‚Üí el modelo explica \\~88% de la variabilidad en el precio.\n",
    "* **RMSE = 1.8053** ‚Üí error promedio bastante bajo.\n",
    "* Todas las variables (excepto quiz√°s `Kms_Driven` y `Owner`) tienen **coeficientes distintos de cero** y con cierta significancia estad√≠stica.\n",
    "* `Fuel_Type`, `Seller_Type`, `Transmission` y `Owner` parecen aportar algo de informaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "##  **Modelo 2: Lasso**\n",
    "\n",
    "* **R¬≤ = 0.8428** ‚Üí explica un poco menos (\\~84%).\n",
    "* **RMSE = 2.0659** ‚Üí el error es un poco mayor.\n",
    "* Muchos coeficientes quedaron en **cero exacto**:\n",
    "\n",
    "  * `Fuel_Type`, `Seller_Type`, `Transmission`, `Owner` fueron eliminados.\n",
    "* En la pr√°ctica, Lasso se qued√≥ con lo ‚Äúesencial‚Äù: `Year`, `Present_Price`, y (en menor medida) `Kms_Driven`.\n",
    "\n",
    "---\n",
    "\n",
    "##  **¬øCu√°l conviene?**\n",
    "\n",
    "Depende del objetivo:\n",
    "\n",
    "1. **Si quer√©s m√°xima capacidad predictiva (menor error):**\n",
    "   ‚Üí Te conviene la **Regresi√≥n Lineal OLS** (porque tiene mejor R¬≤ y menor RMSE).\n",
    "\n",
    "2. **Si quer√©s simplicidad e interpretabilidad (modelo m√°s ‚Äúlimpio‚Äù con menos variables):**\n",
    "   ‚Üí Te conviene el **Lasso**, porque elimina autom√°ticamente variables con poco aporte.\n",
    "\n",
    "3. **Si hay riesgo de sobreajuste (overfitting):**\n",
    "\n",
    "   * OLS puede sobreajustar si ten√©s muchas variables correlacionadas.\n",
    "   * Lasso reduce ese riesgo al forzar algunos coeficientes a cero.\n",
    "\n",
    "---\n",
    "\n",
    "## Recomendaci√≥n pr√°ctica\n",
    "\n",
    "* Prob√° con **validaci√≥n cruzada** (`cross_val_score` en `scikit-learn`) para medir el desempe√±o real fuera de la muestra.\n",
    "* Si la diferencia de error entre OLS y Lasso no es grande, **qu√©date con Lasso**, porque el modelo es m√°s sencillo y generaliza mejor.\n",
    "* Si necesit√°s m√°xima precisi√≥n en tu dataset actual y no te importa la complejidad, **qu√©date con OLS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc5f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "77695-data-science-i-flex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
