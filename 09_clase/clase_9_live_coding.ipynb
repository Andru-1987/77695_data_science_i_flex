{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_selection import RFE, RFECV, SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be53b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wines_path:str= r\"https://docs.google.com/spreadsheets/d/e/2PACX-1vTH3jye4YrhpWRVtWivYX0fxP9VsyXhGxGqTeJ-_LVVxKAuF0xAHImhsLMSwXUi4fRcBP0ETEVZtVRf/pub?output=csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(wines_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AN√ÅLISIS INICIAL DEL DATASET\")\n",
    "\n",
    "# Informaci√≥n b√°sica\n",
    "print(f\"Dimensiones del dataset: {df.shape}\")\n",
    "print(f\"\\nPrimeras 5 filas:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e83b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\nInformaci√≥n del dataset:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed7005",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nEstad√≠sticas descriptivas:\")\n",
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nValores nulos por columna:\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay variables categ√≥ricas\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "print(f\"\\nColumnas categ√≥ricas: {list(categorical_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacbebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los datos\n",
    "def prepare_data(df:pd.DataFrame, target:str = \"target\"):\n",
    "    \"\"\"Preparar datos para feature selection\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    if df_clean.isnull().sum().sum() > 0:\n",
    "        print(\"Manejar valores nulos\")\n",
    "        # Para caracter√≠sticas num√©ricas\n",
    "        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "        df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].median())\n",
    "        \n",
    "        # Para caracter√≠sticas categ√≥ricas\n",
    "        categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            df_clean[col] = df_clean[col].fillna(df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown')\n",
    "    \n",
    "    # Codificar variables categ√≥ricas\n",
    "    label_encoders = {}\n",
    "\n",
    "    for col in df_clean.select_dtypes(include=['object']).columns:\n",
    "        print(\"Evitar codificar la variable objetivo si es categ√≥rica\")\n",
    "        if col != target: \n",
    "\n",
    "            le = LabelEncoder()\n",
    "            df_clean[col] = le.fit_transform(df_clean[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.quality.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fbc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a77280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "df_processed = prepare_data(df,target_column)\n",
    "df_processed.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527507bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar caracter√≠sticas y variable objetivo\n",
    "X = df_processed.drop(columns=[target_column])\n",
    "y = df_processed[target_column]\n",
    "\n",
    "# Si la variable objetivo es categ√≥rica, codificarla\n",
    "if y.dtype == 'object':\n",
    "    le_target = LabelEncoder()\n",
    "    y = le_target.fit_transform(y)\n",
    "    print(f\"Variable objetivo codificada. Clases: {le_target.classes_}\")\n",
    "\n",
    "print(f\"\\nCaracter√≠sticas: {X.shape[1]}\")\n",
    "print(f\"Tama√±o del dataset: {X.shape[0]} muestras\")\n",
    "print(f\"Distribuci√≥n de la variable objetivo: {pd.Series(y).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de55b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar caracter√≠sticas num√©ricas\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278203a",
   "metadata": {},
   "source": [
    "*Recursive Feature Elimination (RFE) con Validaci√≥n Cruzada*\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    # Modelos de manera aleatoria para uso de ver que es mas conveniente de manera random\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc67220",
   "metadata": {},
   "source": [
    "# **Feature Selection: Explicaci√≥n Sencilla**\n",
    "\n",
    "## **RFE (Recursive Feature Elimination) - \"Eliminaci√≥n hacia Atr√°s Inteligente\"**\n",
    "\n",
    "### **¬øC√≥mo funciona?**\n",
    "Imagina que est√°s preparando una mochila para un viaje:\n",
    "\n",
    "1. **Empiezas con todas tus cosas** (todas las caracter√≠sticas)\n",
    "2. **Vas probando** qu√© pasa si sacas cada cosa\n",
    "3. **Eliminas lo que menos afecta** (la caracter√≠stica menos importante)\n",
    "4. **Repites** el proceso hasta quedarte con lo esencial\n",
    "\n",
    "### **Proceso paso a paso:**\n",
    "```\n",
    "Todas las caracter√≠sticas ‚Üí Modelo ‚Üí Identifica la menos importante ‚Üí Elimina ‚Üí Repite\n",
    "```\n",
    "\n",
    "### **Ejemplo pr√°ctico:**\n",
    "**Predecir precio de casas:**\n",
    "- Empiezas con: tama√±o, habitaciones, ba√±os, garage, jard√≠n, piscina\n",
    "- El modelo dice: \"piscina\" es lo menos importante\n",
    "- Eliminas piscina\n",
    "- Repites: ahora \"jard√≠n\" es lo menos importante\n",
    "- Eliminas jard√≠n\n",
    "- Te quedas con: tama√±o, habitaciones, ba√±os, garage\n",
    "\n",
    "### **Ventaja:**\n",
    "- Elimina caracter√≠sticas de forma inteligente\n",
    "- Se enfoca en lo que realmente importa\n",
    "\n",
    "---\n",
    "\n",
    "## **Forward Selection - \"Construcci√≥n desde Cero\"**\n",
    "\n",
    "### **¬øC√≥mo funciona?**\n",
    "Como armar un equipo de f√∫tbol empezando desde cero:\n",
    "\n",
    "1. **Empiezas con equipo vac√≠o** (ninguna caracter√≠stica)\n",
    "2. **Pruebas cada jugador individualmente** (cada caracter√≠stica sola)\n",
    "3. **Agregas al mejor jugador** (la caracter√≠stica que m√°s mejora el modelo)\n",
    "4. **Repites** buscando qui√©n complementa mejor al equipo\n",
    "\n",
    "### **Proceso paso a paso:**\n",
    "```\n",
    "Caracter√≠stica vac√≠a ‚Üí Prueba cada una ‚Üí Agrega la mejor ‚Üí Prueba combinaciones ‚Üí Repite\n",
    "```\n",
    "\n",
    "### **Ejemplo pr√°ctico:**\n",
    "**Predecir si un cliente comprar√°:**\n",
    "- Pruebas individual: \"edad\" ‚Üí buen resultado\n",
    "- Pruebas individual: \"ingresos\" ‚Üí mejor resultado\n",
    "- Agregas \"ingresos\" (la mejor)\n",
    "- Ahora pruebas: \"ingresos + edad\", \"ingresos + historial\", \"ingresos + ubicaci√≥n\"\n",
    "- \"ingresos + historial\" da mejor resultado ‚Üí agregas historial\n",
    "- Continuas...\n",
    "\n",
    "### **Ventaja:**\n",
    "- No pierdes caracter√≠sticas importantes\n",
    "- Computacionalmente m√°s eficiente al empezar con pocas\n",
    "\n",
    "---\n",
    "\n",
    "## **Backward Elimination - \"Limpieza General\"**\n",
    "\n",
    "### **¬øC√≥mo funciona?**\n",
    "Como limpiar tu closet:\n",
    "\n",
    "1. **Tienes toda la ropa** (todas las caracter√≠sticas)\n",
    "2. **Ves qu√© prenda usas menos** (caracter√≠stica menos √∫til)\n",
    "3. **La sacas del closet** (eliminas)\n",
    "4. **Verificas** si todav√≠a puedes vestirte bien\n",
    "5. **Repites** hasta quedarte con lo esencial\n",
    "\n",
    "### **Proceso paso a paso:**\n",
    "```\n",
    "Todas las caracter√≠sticas ‚Üí Elimina la menos importante ‚Üí Verifica rendimiento ‚Üí Repite\n",
    "```\n",
    "\n",
    "### **Ejemplo pr√°ctico:**\n",
    "**Diagnosticar una enfermedad:**\n",
    "- Tienes: fiebre, tos, dolor cabeza, cansancio, estornudos, dolor muscular\n",
    "- El modelo dice: \"estornudos\" es lo menos relevante\n",
    "- Eliminas estornudos\n",
    "- Ahora: \"dolor cabeza\" es lo menos relevante\n",
    "- Eliminas dolor cabeza\n",
    "- Te quedas con: fiebre, tos, cansancio, dolor muscular\n",
    "\n",
    "### **Ventaja:**\n",
    "- Menos riesgo de perder combinaciones importantes\n",
    "- Bueno cuando tienes muchas caracter√≠sticas\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparaci√≥n Sencilla**\n",
    "\n",
    "| M√©todo | Analog√≠a | Cu√°ndo usar | Velocidad |\n",
    "|--------|----------|-------------|-----------|\n",
    "| **RFE** | Eliminar lo menos √∫til de tu mochila | Cuando quieres selecci√≥n autom√°tica | üü° Media |\n",
    "| **Forward** | Construir equipo desde cero | Cuando tienes muchas caracter√≠sticas | üü¢ R√°pido al inicio |\n",
    "| **Backward** | Limpiar closet lleno | Cuando tienes pocas caracter√≠sticas | üî¥ Lento al inicio |\n",
    "\n",
    "---\n",
    "\n",
    "## **¬øCu√°l elegir?**\n",
    "\n",
    "### **Forward Selection es mejor cuando:**\n",
    "- Tienes MUCHAS caracter√≠sticas (100+)\n",
    "- Quieres empezar r√°pido\n",
    "- Crees que pocas caracter√≠sticas son suficientes\n",
    "\n",
    "### **Backward Elimination es mejor cuando:**\n",
    "- Tienes POCAS caracter√≠sticas (menos de 50)\n",
    "- No quieres perder combinaciones importantes\n",
    "- Tienes tiempo computacional\n",
    "\n",
    "### **RFE es mejor cuando:**\n",
    "- Quieres un balance entre ambos\n",
    "- Necesitas que el m√©todo decida cu√°ntas caracter√≠sticas mantener\n",
    "- Buscas robustez\n",
    "\n",
    "## **TIPS**\n",
    "\n",
    "- **Forward**: \"¬øQu√© debo AGREGAR?\"\n",
    "- **Backward**: \"¬øQu√© debo ELIMINAR?\"  \n",
    "- **RFE**: \"¬øQu√© es lo MENOS importante?\"\n",
    "\n",
    "Todos buscan lo mismo: quedarse con las caracter√≠sticas que realmente ayudan a predecir, eliminando el \"ruido\" que no aporta valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rfecv_results(results, feature_names):\n",
    "    \n",
    "    if not results['success']:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        cv_results = results['cv_results']\n",
    "        \n",
    "        # N√∫mero total de puntos evaluados\n",
    "        n_scores = len(cv_results['mean_test_score'])\n",
    "        n_features_range = range(1, n_scores + 1)\n",
    "        \n",
    "        print(f\"   üìä Debug: √ìptimo reportado = {results['n_features_optimal']}, Puntos evaluados = {n_scores}\")\n",
    "        \n",
    "        # L√≠nea principal con scores\n",
    "        plt.plot(n_features_range, cv_results['mean_test_score'], \n",
    "                 label='Score promedio', linewidth=2, color='blue')\n",
    "        \n",
    "        # √Årea de incertidumbre\n",
    "        plt.fill_between(n_features_range,\n",
    "                        cv_results['mean_test_score'] - cv_results['std_test_score'],\n",
    "                        cv_results['mean_test_score'] + cv_results['std_test_score'],\n",
    "                        alpha=0.2, label='¬± 1 desviaci√≥n est√°ndar', color='blue')\n",
    "        \n",
    "        # Encontrar el √≠ndice real del mejor score\n",
    "        best_idx = np.argmax(cv_results['mean_test_score'])\n",
    "        best_n_features = best_idx + 1  # +1 porque los √≠ndices empiezan en 0\n",
    "        best_score = cv_results['mean_test_score'][best_idx]\n",
    "        \n",
    "        # L√≠nea vertical en el punto √≥ptimo REAL (el que est√° en el gr√°fico)\n",
    "        plt.axvline(x=best_n_features, \n",
    "                   color='red', linestyle='--', \n",
    "                   label=f'√ìptimo real: {best_n_features} caracter√≠sticas')\n",
    "        \n",
    "        # Punto destacado en el √≥ptimo real\n",
    "        plt.scatter(best_n_features, best_score,\n",
    "                   color='red', s=100, zorder=5, label='Punto √≥ptimo')\n",
    "        \n",
    "        # Si hay discrepancia, mostrarla\n",
    "        if results['n_features_optimal'] != best_n_features:\n",
    "            plt.axvline(x=results['n_features_optimal'], \n",
    "                       color='orange', linestyle=':', \n",
    "                       label=f'√ìptimo reportado: {results[\"n_features_optimal\"]} caracter√≠sticas',\n",
    "                       alpha=0.7)\n",
    "        \n",
    "        plt.xlabel('N√∫mero de caracter√≠sticas')\n",
    "        plt.ylabel('Accuracy Score')\n",
    "        plt.title(f'RFECV - {results[\"model_name\"]}\\n'\n",
    "                 f'√ìptimo real: {best_n_features} caracter√≠sticas | '\n",
    "                 f'Mejor score: {best_score:.4f}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Informaci√≥n de debug\n",
    "        if results['n_features_optimal'] != best_n_features:\n",
    "            print(f\"   ‚ö†Ô∏è  Discrepancia: RFECV reporta {results['n_features_optimal']} √≥ptimas,\"\n",
    "                  f\" pero el mejor score est√° en {best_n_features}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error al graficar {results['model_name']}: {e}\")\n",
    "\n",
    "\n",
    "def run_rfecv_analysis(model, model_name, X, y, min_features=5, cv_folds=5, step=1):\n",
    "\n",
    "    print(f\"\\nüîç {model_name}:\")\n",
    "    \n",
    "    try:\n",
    "        # Calcular el step para que no haya demasiados puntos\n",
    "        n_features = X.shape[1]\n",
    "        if step == 'auto':\n",
    "            step = max(1, n_features // 20)  # M√°ximo 20 puntos en el gr√°fico\n",
    "        \n",
    "        # Configurar y ejecutar RFECV\n",
    "        rfecv = RFECV(\n",
    "            estimator=model,\n",
    "            step=step,\n",
    "            cv=StratifiedKFold(cv_folds),\n",
    "            scoring='accuracy',\n",
    "            min_features_to_select=min_features,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rfecv.fit(X, y)\n",
    "        \n",
    "        # Informaci√≥n de debug\n",
    "        n_evaluated = len(rfecv.cv_results_['mean_test_score'])\n",
    "        print(f\">>> Caracter√≠sticas totales: {n_features}\")\n",
    "        print(f\">>> Puntos evaluados: {n_evaluated}\")\n",
    "        print(f\">>> Step usado: {step}\")\n",
    "        \n",
    "        # Encontrar el mejor score real (puede diferir del reportado)\n",
    "        best_idx = np.argmax(rfecv.cv_results_['mean_test_score'])\n",
    "        best_n_features_actual = best_idx + 1\n",
    "        best_score_actual = rfecv.cv_results_['mean_test_score'][best_idx]\n",
    "        \n",
    "        # Recopilar resultados\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'n_features_optimal': rfecv.n_features_,\n",
    "            'n_features_optimal_actual': best_n_features_actual,\n",
    "            'best_score': rfecv.cv_results_['mean_test_score'].max(),\n",
    "            'best_score_actual': best_score_actual,\n",
    "            'selected_features': X.columns[rfecv.support_],\n",
    "            'feature_ranking': rfecv.ranking_,\n",
    "            'cv_results': rfecv.cv_results_,\n",
    "            'rfecv_object': rfecv,\n",
    "            'n_features_evaluated': n_evaluated,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        print(f\">>> N√∫mero √≥ptimo reportado: {results['n_features_optimal']}\")\n",
    "        print(f\">>> N√∫mero √≥ptimo real: {results['n_features_optimal_actual']}\")\n",
    "        print(f\">>> Mejor score de validaci√≥n: {results['best_score']:.4f}\")\n",
    "        print(f\">>> Caracter√≠sticas seleccionadas ({len(results['selected_features'])}):\")\n",
    "        for i, feature in enumerate(results['selected_features'], 1):\n",
    "            print(f\"     {i}. {feature}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"   ‚ùå Error con {model_name}: {e}\"\n",
    "        print(error_msg)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    # Ejecutar RFECV\n",
    "    resultados_rf = run_rfecv_analysis(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        X=X_scaled,  # Tus caracter√≠sticas escaladas\n",
    "        y=y,         # Tu variable objetivo\n",
    "        min_features=5,\n",
    "        cv_folds=5\n",
    "    )\n",
    "\n",
    "    plot_rfecv_results(resultados_rf, X_scaled.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8cee8",
   "metadata": {},
   "source": [
    "**Forward Selection - Implementaci√≥n Completa**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_forward_selection(model, model_name, X, y, n_features='auto', cv_folds=5, direction='forward'):\n",
    "  \n",
    "    print(f\"\\n{model_name} - Forward Selection:\")\n",
    "    \n",
    "    try:\n",
    "        # Si es 'auto', selecciona la mitad de las caracter√≠sticas\n",
    "        if n_features == 'auto':\n",
    "            n_features = max(1, X.shape[1] // 2)\n",
    "        \n",
    "        # Configurar Forward Selection\n",
    "        sfs_forward = SequentialFeatureSelector(\n",
    "            estimator=model,\n",
    "            n_features_to_select=n_features,\n",
    "            direction=direction,\n",
    "            cv=StratifiedKFold(cv_folds),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Ajustar el selector\n",
    "        sfs_forward.fit(X, y)\n",
    "        \n",
    "        # Obtener caracter√≠sticas seleccionadas\n",
    "        selected_features = X.columns[sfs_forward.get_support()]\n",
    "        feature_mask = sfs_forward.get_support()\n",
    "        \n",
    "        # Evaluar performance con las caracter√≠sticas seleccionadas\n",
    "        X_selected = sfs_forward.transform(X)\n",
    "        cv_scores = cross_val_score(model, X_selected, y, cv=cv_folds, scoring='accuracy')\n",
    "        \n",
    "        # Recopilar resultados\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'method': 'Forward Selection',\n",
    "            'n_features_selected': len(selected_features),\n",
    "            'selected_features': selected_features,\n",
    "            'feature_mask': feature_mask,\n",
    "            'mean_score': cv_scores.mean(),\n",
    "            'std_score': cv_scores.std(),\n",
    "            'cv_scores': cv_scores,\n",
    "            'sfs_object': sfs_forward,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        print(f\">>> Caracter√≠sticas seleccionadas: {results['n_features_selected']}\")\n",
    "        print(f\">>> Score de validaci√≥n: {results['mean_score']:.4f} ¬± {results['std_score']:.4f}\")\n",
    "        print(f\">>> Caracter√≠sticas:\")\n",
    "        for i, feature in enumerate(results['selected_features'], 1):\n",
    "            print(f\"     {i}. {feature}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"   ‚ùå Error con {model_name}: {e}\"\n",
    "        print(error_msg)\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'method': 'Forward Selection',\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def plot_forward_selection_results(results, X_original):\n",
    "    \"\"\"\n",
    "    Genera gr√°fico comparativo para Forward Selection\n",
    "    \"\"\"\n",
    "    if not results['success']:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Comparar performance vs todas las caracter√≠sticas\n",
    "        model = results['sfs_object'].estimator\n",
    "        all_features_score = cross_val_score(model, X_original, y, cv=5, scoring='accuracy')\n",
    "        \n",
    "        # Preparar datos para el gr√°fico\n",
    "        methods = ['Todas las caracter√≠sticas', 'Forward Selection']\n",
    "        scores = [all_features_score.mean(), results['mean_score']]\n",
    "        errors = [all_features_score.std(), results['std_score']]\n",
    "        \n",
    "        # Crear gr√°fico de comparaci√≥n\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(methods, scores, yerr=errors, capsize=10, \n",
    "                      color=['lightblue', 'lightgreen'], alpha=0.7)\n",
    "        \n",
    "        # A√±adir valores en las barras\n",
    "        for i, (score, error) in enumerate(zip(scores, errors)):\n",
    "            plt.text(i, score + error + 0.01, f'{score:.4f} ¬± {error:.4f}', \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.ylabel('Accuracy Score')\n",
    "        plt.title(f'Forward Selection - {results[\"model_name\"]}\\n'\n",
    "                 f'Reducci√≥n: {X_original.shape[1]} ‚Üí {results[\"n_features_selected\"]} caracter√≠sticas')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\" Comparaci√≥n:\")\n",
    "        print(f\"      ‚Ä¢ Todas las caracter√≠sticas ({X_original.shape[1]}): {all_features_score.mean():.4f}\")\n",
    "        print(f\"      ‚Ä¢ Forward Selection ({results['n_features_selected']}): {results['mean_score']:.4f}\")\n",
    "        print(f\"      ‚Ä¢ Mejora: {results['mean_score'] - all_features_score.mean():+.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error al graficar {results['model_name']}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b94978",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "\n",
    "    resultados = run_forward_selection(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        X=X_scaled,  # Tu DataFrame de caracter√≠sticas\n",
    "        y=y,         # Tu variable objetivo\n",
    "        n_features='auto',  # Selecciona autom√°ticamente la mitad\n",
    "        cv_folds=5\n",
    "    )\n",
    "        \n",
    "    # Generar gr√°fico comparativo\n",
    "    plot_forward_selection_results(resultados, X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e75b309",
   "metadata": {},
   "source": [
    "**Backward Elimination - Implementaci√≥n Completa**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283108e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backward_elimination(model, model_name, X, y, n_features='auto', cv_folds=5):\n",
    "\n",
    "    print(f\"\\n{model_name} - Backward Elimination:\")\n",
    "    \n",
    "    try:\n",
    "        # Si es 'auto', selecciona la mitad de las caracter√≠sticas\n",
    "        if n_features == 'auto':\n",
    "            n_features = max(1, X.shape[1] // 2)\n",
    "        \n",
    "        # Configurar Backward Elimination\n",
    "        sfs_backward = SequentialFeatureSelector(\n",
    "            estimator=model,\n",
    "            n_features_to_select=n_features,\n",
    "            direction='backward',\n",
    "            cv=StratifiedKFold(cv_folds),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Ajustar el selector\n",
    "        sfs_backward.fit(X, y)\n",
    "        \n",
    "        # Obtener caracter√≠sticas seleccionadas\n",
    "        selected_features = X.columns[sfs_backward.get_support()]\n",
    "        feature_mask = sfs_backward.get_support()\n",
    "        \n",
    "        # Evaluar performance con las caracter√≠sticas seleccionadas\n",
    "        X_selected = sfs_backward.transform(X)\n",
    "        cv_scores = cross_val_score(model, X_selected, y, cv=cv_folds, scoring='accuracy')\n",
    "        \n",
    "        # Recopilar resultados\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'method': 'Backward Elimination',\n",
    "            'n_features_selected': len(selected_features),\n",
    "            'selected_features': selected_features,\n",
    "            'feature_mask': feature_mask,\n",
    "            'mean_score': cv_scores.mean(),\n",
    "            'std_score': cv_scores.std(),\n",
    "            'cv_scores': cv_scores,\n",
    "            'sfs_object': sfs_backward,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        print(f\">>> Caracter√≠sticas seleccionadas: {results['n_features_selected']}\")\n",
    "        print(f\">>> Score de validaci√≥n: {results['mean_score']:.4f} ¬± {results['std_score']:.4f}\")\n",
    "        print(f\">>> Caracter√≠sticas:\")\n",
    "        for i, feature in enumerate(results['selected_features'], 1):\n",
    "            print(f\"     {i}. {feature}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"   ‚ùå Error con {model_name}: {e}\"\n",
    "        print(error_msg)\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'method': 'Backward Elimination',\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def plot_backward_elimination_results(results, X_original):\n",
    "\n",
    "    if not results['success']:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Comparar performance vs todas las caracter√≠sticas\n",
    "        model = results['sfs_object'].estimator\n",
    "        all_features_score = cross_val_score(model, X_original, y, cv=5, scoring='accuracy')\n",
    "        \n",
    "        # Preparar datos para el gr√°fico\n",
    "        methods = ['Todas las caracter√≠sticas', 'Backward Elimination']\n",
    "        scores = [all_features_score.mean(), results['mean_score']]\n",
    "        errors = [all_features_score.std(), results['std_score']]\n",
    "        \n",
    "        # Crear gr√°fico de comparaci√≥n\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(methods, scores, yerr=errors, capsize=10, \n",
    "                      color=['lightblue', 'lightcoral'], alpha=0.7)\n",
    "        \n",
    "        # A√±adir valores en las barras\n",
    "        for i, (score, error) in enumerate(zip(scores, errors)):\n",
    "            plt.text(i, score + error + 0.01, f'{score:.4f} ¬± {error:.4f}', \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.ylabel('Accuracy Score')\n",
    "        plt.title(f'Backward Elimination - {results[\"model_name\"]}\\n'\n",
    "                 f'Reducci√≥n: {X_original.shape[1]} ‚Üí {results[\"n_features_selected\"]} caracter√≠sticas')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\" Comparaci√≥n:\")\n",
    "        print(f\"      ‚Ä¢ Todas las caracter√≠sticas ({X_original.shape[1]}): {all_features_score.mean():.4f}\")\n",
    "        print(f\"      ‚Ä¢ Backward Elimination ({results['n_features_selected']}): {results['mean_score']:.4f}\")\n",
    "        print(f\"      ‚Ä¢ Mejora: {results['mean_score'] - all_features_score.mean():+.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error al graficar {results['model_name']}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3452da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    resultados = run_backward_elimination(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )\n",
    "    \n",
    "    \n",
    "    plot_forward_selection_results(resultados, X)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "77695-data-science-i-flex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
