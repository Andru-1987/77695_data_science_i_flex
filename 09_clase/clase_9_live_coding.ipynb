{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_selection import RFE, RFECV, SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be53b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wines_path:str= r\"https://docs.google.com/spreadsheets/d/e/2PACX-1vTH3jye4YrhpWRVtWivYX0fxP9VsyXhGxGqTeJ-_LVVxKAuF0xAHImhsLMSwXUi4fRcBP0ETEVZtVRf/pub?output=csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(wines_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANÁLISIS INICIAL DEL DATASET\")\n",
    "\n",
    "# Información básica\n",
    "print(f\"Dimensiones del dataset: {df.shape}\")\n",
    "print(f\"\\nPrimeras 5 filas:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e83b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\nInformación del dataset:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed7005",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nEstadísticas descriptivas:\")\n",
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nValores nulos por columna:\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay variables categóricas\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "print(f\"\\nColumnas categóricas: {list(categorical_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacbebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los datos\n",
    "def prepare_data(df:pd.DataFrame, target:str = \"target\"):\n",
    "    \"\"\"Preparar datos para feature selection\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    if df_clean.isnull().sum().sum() > 0:\n",
    "        print(\"Manejar valores nulos\")\n",
    "        # Para características numéricas\n",
    "        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "        df_clean[numeric_cols] = df_clean[numeric_cols].fillna(df_clean[numeric_cols].median())\n",
    "        \n",
    "        # Para características categóricas\n",
    "        categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            df_clean[col] = df_clean[col].fillna(df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown')\n",
    "    \n",
    "    # Codificar variables categóricas\n",
    "    label_encoders = {}\n",
    "\n",
    "    for col in df_clean.select_dtypes(include=['object']).columns:\n",
    "        print(\"Evitar codificar la variable objetivo si es categórica\")\n",
    "        if col != target: \n",
    "\n",
    "            le = LabelEncoder()\n",
    "            df_clean[col] = le.fit_transform(df_clean[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.quality.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fbc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a77280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "df_processed = prepare_data(df,target_column)\n",
    "df_processed.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527507bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características y variable objetivo\n",
    "X = df_processed.drop(columns=[target_column])\n",
    "y = df_processed[target_column]\n",
    "\n",
    "# Si la variable objetivo es categórica, codificarla\n",
    "if y.dtype == 'object':\n",
    "    le_target = LabelEncoder()\n",
    "    y = le_target.fit_transform(y)\n",
    "    print(f\"Variable objetivo codificada. Clases: {le_target.classes_}\")\n",
    "\n",
    "print(f\"\\nCaracterísticas: {X.shape[1]}\")\n",
    "print(f\"Tamaño del dataset: {X.shape[0]} muestras\")\n",
    "print(f\"Distribución de la variable objetivo: {pd.Series(y).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de55b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar características numéricas\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278203a",
   "metadata": {},
   "source": [
    "*Recursive Feature Elimination (RFE) con Validación Cruzada*\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    # Modelos de manera aleatoria para uso de ver que es mas conveniente de manera random\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc67220",
   "metadata": {},
   "source": [
    "# **Feature Selection: Explicación Sencilla**\n",
    "\n",
    "## **RFE (Recursive Feature Elimination) - \"Eliminación hacia Atrás Inteligente\"**\n",
    "\n",
    "### **¿Cómo funciona?**\n",
    "Imagina que estás preparando una mochila para un viaje:\n",
    "\n",
    "1. **Empiezas con todas tus cosas** (todas las características)\n",
    "2. **Vas probando** qué pasa si sacas cada cosa\n",
    "3. **Eliminas lo que menos afecta** (la característica menos importante)\n",
    "4. **Repites** el proceso hasta quedarte con lo esencial\n",
    "\n",
    "### **Proceso paso a paso:**\n",
    "```\n",
    "Todas las características → Modelo → Identifica la menos importante → Elimina → Repite\n",
    "```\n",
    "\n",
    "### **Ejemplo práctico:**\n",
    "**Predecir precio de casas:**\n",
    "- Empiezas con: tamaño, habitaciones, baños, garage, jardín, piscina\n",
    "- El modelo dice: \"piscina\" es lo menos importante\n",
    "- Eliminas piscina\n",
    "- Repites: ahora \"jardín\" es lo menos importante\n",
    "- Eliminas jardín\n",
    "- Te quedas con: tamaño, habitaciones, baños, garage\n",
    "\n",
    "### **Ventaja:**\n",
    "- Elimina características de forma inteligente\n",
    "- Se enfoca en lo que realmente importa\n",
    "\n",
    "---\n",
    "\n",
    "## **Forward Selection - \"Construcción desde Cero\"**\n",
    "\n",
    "### **¿Cómo funciona?**\n",
    "Como armar un equipo de fútbol empezando desde cero:\n",
    "\n",
    "1. **Empiezas con equipo vacío** (ninguna característica)\n",
    "2. **Pruebas cada jugador individualmente** (cada característica sola)\n",
    "3. **Agregas al mejor jugador** (la característica que más mejora el modelo)\n",
    "4. **Repites** buscando quién complementa mejor al equipo\n",
    "\n",
    "### **Proceso paso a paso:**\n",
    "```\n",
    "Característica vacía → Prueba cada una → Agrega la mejor → Prueba combinaciones → Repite\n",
    "```\n",
    "\n",
    "### **Ejemplo práctico:**\n",
    "**Predecir si un cliente comprará:**\n",
    "- Pruebas individual: \"edad\" → buen resultado\n",
    "- Pruebas individual: \"ingresos\" → mejor resultado\n",
    "- Agregas \"ingresos\" (la mejor)\n",
    "- Ahora pruebas: \"ingresos + edad\", \"ingresos + historial\", \"ingresos + ubicación\"\n",
    "- \"ingresos + historial\" da mejor resultado → agregas historial\n",
    "- Continuas...\n",
    "\n",
    "### **Ventaja:**\n",
    "- No pierdes características importantes\n",
    "- Computacionalmente más eficiente al empezar con pocas\n",
    "\n",
    "---\n",
    "\n",
    "## **Backward Elimination - \"Limpieza General\"**\n",
    "\n",
    "### **¿Cómo funciona?**\n",
    "Como limpiar tu closet:\n",
    "\n",
    "1. **Tienes toda la ropa** (todas las características)\n",
    "2. **Ves qué prenda usas menos** (característica menos útil)\n",
    "3. **La sacas del closet** (eliminas)\n",
    "4. **Verificas** si todavía puedes vestirte bien\n",
    "5. **Repites** hasta quedarte con lo esencial\n",
    "\n",
    "### **Proceso paso a paso:**\n",
    "```\n",
    "Todas las características → Elimina la menos importante → Verifica rendimiento → Repite\n",
    "```\n",
    "\n",
    "### **Ejemplo práctico:**\n",
    "**Diagnosticar una enfermedad:**\n",
    "- Tienes: fiebre, tos, dolor cabeza, cansancio, estornudos, dolor muscular\n",
    "- El modelo dice: \"estornudos\" es lo menos relevante\n",
    "- Eliminas estornudos\n",
    "- Ahora: \"dolor cabeza\" es lo menos relevante\n",
    "- Eliminas dolor cabeza\n",
    "- Te quedas con: fiebre, tos, cansancio, dolor muscular\n",
    "\n",
    "### **Ventaja:**\n",
    "- Menos riesgo de perder combinaciones importantes\n",
    "- Bueno cuando tienes muchas características\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparación Sencilla**\n",
    "\n",
    "| Método | Analogía | Cuándo usar | Velocidad |\n",
    "|--------|----------|-------------|-----------|\n",
    "| **RFE** | Eliminar lo menos útil de tu mochila | Cuando quieres selección automática | 🟡 Media |\n",
    "| **Forward** | Construir equipo desde cero | Cuando tienes muchas características | 🟢 Rápido al inicio |\n",
    "| **Backward** | Limpiar closet lleno | Cuando tienes pocas características | 🔴 Lento al inicio |\n",
    "\n",
    "---\n",
    "\n",
    "## **¿Cuál elegir?**\n",
    "\n",
    "### **Forward Selection es mejor cuando:**\n",
    "- Tienes MUCHAS características (100+)\n",
    "- Quieres empezar rápido\n",
    "- Crees que pocas características son suficientes\n",
    "\n",
    "### **Backward Elimination es mejor cuando:**\n",
    "- Tienes POCAS características (menos de 50)\n",
    "- No quieres perder combinaciones importantes\n",
    "- Tienes tiempo computacional\n",
    "\n",
    "### **RFE es mejor cuando:**\n",
    "- Quieres un balance entre ambos\n",
    "- Necesitas que el método decida cuántas características mantener\n",
    "- Buscas robustez\n",
    "\n",
    "## **TIPS**\n",
    "\n",
    "- **Forward**: \"¿Qué debo AGREGAR?\"\n",
    "- **Backward**: \"¿Qué debo ELIMINAR?\"  \n",
    "- **RFE**: \"¿Qué es lo MENOS importante?\"\n",
    "\n",
    "Todos buscan lo mismo: quedarse con las características que realmente ayudan a predecir, eliminando el \"ruido\" que no aporta valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rfecv_results(results, feature_names):\n",
    "    \n",
    "    if not results['success']:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        cv_results = results['cv_results']\n",
    "        \n",
    "        # Número total de puntos evaluados\n",
    "        n_scores = len(cv_results['mean_test_score'])\n",
    "        n_features_range = range(1, n_scores + 1)\n",
    "        \n",
    "        print(f\"   📊 Debug: Óptimo reportado = {results['n_features_optimal']}, Puntos evaluados = {n_scores}\")\n",
    "        \n",
    "        # Línea principal con scores\n",
    "        plt.plot(n_features_range, cv_results['mean_test_score'], \n",
    "                 label='Score promedio', linewidth=2, color='blue')\n",
    "        \n",
    "        # Área de incertidumbre\n",
    "        plt.fill_between(n_features_range,\n",
    "                        cv_results['mean_test_score'] - cv_results['std_test_score'],\n",
    "                        cv_results['mean_test_score'] + cv_results['std_test_score'],\n",
    "                        alpha=0.2, label='± 1 desviación estándar', color='blue')\n",
    "        \n",
    "        # Encontrar el índice real del mejor score\n",
    "        best_idx = np.argmax(cv_results['mean_test_score'])\n",
    "        best_n_features = best_idx + 1  # +1 porque los índices empiezan en 0\n",
    "        best_score = cv_results['mean_test_score'][best_idx]\n",
    "        \n",
    "        # Línea vertical en el punto óptimo REAL (el que está en el gráfico)\n",
    "        plt.axvline(x=best_n_features, \n",
    "                   color='red', linestyle='--', \n",
    "                   label=f'Óptimo real: {best_n_features} características')\n",
    "        \n",
    "        # Punto destacado en el óptimo real\n",
    "        plt.scatter(best_n_features, best_score,\n",
    "                   color='red', s=100, zorder=5, label='Punto óptimo')\n",
    "        \n",
    "        # Si hay discrepancia, mostrarla\n",
    "        if results['n_features_optimal'] != best_n_features:\n",
    "            plt.axvline(x=results['n_features_optimal'], \n",
    "                       color='orange', linestyle=':', \n",
    "                       label=f'Óptimo reportado: {results[\"n_features_optimal\"]} características',\n",
    "                       alpha=0.7)\n",
    "        \n",
    "        plt.xlabel('Número de características')\n",
    "        plt.ylabel('Accuracy Score')\n",
    "        plt.title(f'RFECV - {results[\"model_name\"]}\\n'\n",
    "                 f'Óptimo real: {best_n_features} características | '\n",
    "                 f'Mejor score: {best_score:.4f}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Información de debug\n",
    "        if results['n_features_optimal'] != best_n_features:\n",
    "            print(f\"   ⚠️  Discrepancia: RFECV reporta {results['n_features_optimal']} óptimas,\"\n",
    "                  f\" pero el mejor score está en {best_n_features}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error al graficar {results['model_name']}: {e}\")\n",
    "\n",
    "\n",
    "def run_rfecv_analysis(model, model_name, X, y, min_features=5, cv_folds=5, step=1):\n",
    "\n",
    "    print(f\"\\n🔍 {model_name}:\")\n",
    "    \n",
    "    try:\n",
    "        # Calcular el step para que no haya demasiados puntos\n",
    "        n_features = X.shape[1]\n",
    "        if step == 'auto':\n",
    "            step = max(1, n_features // 20)  # Máximo 20 puntos en el gráfico\n",
    "        \n",
    "        # Configurar y ejecutar RFECV\n",
    "        rfecv = RFECV(\n",
    "            estimator=model,\n",
    "            step=step,\n",
    "            cv=StratifiedKFold(cv_folds),\n",
    "            scoring='accuracy',\n",
    "            min_features_to_select=min_features,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rfecv.fit(X, y)\n",
    "        \n",
    "        # Información de debug\n",
    "        n_evaluated = len(rfecv.cv_results_['mean_test_score'])\n",
    "        print(f\">>> Características totales: {n_features}\")\n",
    "        print(f\">>> Puntos evaluados: {n_evaluated}\")\n",
    "        print(f\">>> Step usado: {step}\")\n",
    "        \n",
    "        # Encontrar el mejor score real (puede diferir del reportado)\n",
    "        best_idx = np.argmax(rfecv.cv_results_['mean_test_score'])\n",
    "        best_n_features_actual = best_idx + 1\n",
    "        best_score_actual = rfecv.cv_results_['mean_test_score'][best_idx]\n",
    "        \n",
    "        # Recopilar resultados\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'n_features_optimal': rfecv.n_features_,\n",
    "            'n_features_optimal_actual': best_n_features_actual,\n",
    "            'best_score': rfecv.cv_results_['mean_test_score'].max(),\n",
    "            'best_score_actual': best_score_actual,\n",
    "            'selected_features': X.columns[rfecv.support_],\n",
    "            'feature_ranking': rfecv.ranking_,\n",
    "            'cv_results': rfecv.cv_results_,\n",
    "            'rfecv_object': rfecv,\n",
    "            'n_features_evaluated': n_evaluated,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        print(f\">>> Número óptimo reportado: {results['n_features_optimal']}\")\n",
    "        print(f\">>> Número óptimo real: {results['n_features_optimal_actual']}\")\n",
    "        print(f\">>> Mejor score de validación: {results['best_score']:.4f}\")\n",
    "        print(f\">>> Características seleccionadas ({len(results['selected_features'])}):\")\n",
    "        for i, feature in enumerate(results['selected_features'], 1):\n",
    "            print(f\"     {i}. {feature}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"   ❌ Error con {model_name}: {e}\"\n",
    "        print(error_msg)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    # Ejecutar RFECV\n",
    "    resultados_rf = run_rfecv_analysis(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        X=X_scaled,  # Tus características escaladas\n",
    "        y=y,         # Tu variable objetivo\n",
    "        min_features=5,\n",
    "        cv_folds=5\n",
    "    )\n",
    "\n",
    "    plot_rfecv_results(resultados_rf, X_scaled.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8cee8",
   "metadata": {},
   "source": [
    "**Forward Selection - Implementación Completa**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_forward_selection(model, model_name, X, y, n_features='auto', cv_folds=5, direction='forward'):\n",
    "  \n",
    "    print(f\"\\n{model_name} - Forward Selection:\")\n",
    "    \n",
    "    try:\n",
    "        # Si es 'auto', selecciona la mitad de las características\n",
    "        if n_features == 'auto':\n",
    "            n_features = max(1, X.shape[1] // 2)\n",
    "        \n",
    "        # Configurar Forward Selection\n",
    "        sfs_forward = SequentialFeatureSelector(\n",
    "            estimator=model,\n",
    "            n_features_to_select=n_features,\n",
    "            direction=direction,\n",
    "            cv=StratifiedKFold(cv_folds),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Ajustar el selector\n",
    "        sfs_forward.fit(X, y)\n",
    "        \n",
    "        # Obtener características seleccionadas\n",
    "        selected_features = X.columns[sfs_forward.get_support()]\n",
    "        feature_mask = sfs_forward.get_support()\n",
    "        \n",
    "        # Evaluar performance con las características seleccionadas\n",
    "        X_selected = sfs_forward.transform(X)\n",
    "        cv_scores = cross_val_score(model, X_selected, y, cv=cv_folds, scoring='accuracy')\n",
    "        \n",
    "        # Recopilar resultados\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'method': 'Forward Selection',\n",
    "            'n_features_selected': len(selected_features),\n",
    "            'selected_features': selected_features,\n",
    "            'feature_mask': feature_mask,\n",
    "            'mean_score': cv_scores.mean(),\n",
    "            'std_score': cv_scores.std(),\n",
    "            'cv_scores': cv_scores,\n",
    "            'sfs_object': sfs_forward,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        print(f\">>> Características seleccionadas: {results['n_features_selected']}\")\n",
    "        print(f\">>> Score de validación: {results['mean_score']:.4f} ± {results['std_score']:.4f}\")\n",
    "        print(f\">>> Características:\")\n",
    "        for i, feature in enumerate(results['selected_features'], 1):\n",
    "            print(f\"     {i}. {feature}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"   ❌ Error con {model_name}: {e}\"\n",
    "        print(error_msg)\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'method': 'Forward Selection',\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def plot_forward_selection_results(results, X_original):\n",
    "    \"\"\"\n",
    "    Genera gráfico comparativo para Forward Selection\n",
    "    \"\"\"\n",
    "    if not results['success']:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Comparar performance vs todas las características\n",
    "        model = results['sfs_object'].estimator\n",
    "        all_features_score = cross_val_score(model, X_original, y, cv=5, scoring='accuracy')\n",
    "        \n",
    "        # Preparar datos para el gráfico\n",
    "        methods = ['Todas las características', 'Forward Selection']\n",
    "        scores = [all_features_score.mean(), results['mean_score']]\n",
    "        errors = [all_features_score.std(), results['std_score']]\n",
    "        \n",
    "        # Crear gráfico de comparación\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(methods, scores, yerr=errors, capsize=10, \n",
    "                      color=['lightblue', 'lightgreen'], alpha=0.7)\n",
    "        \n",
    "        # Añadir valores en las barras\n",
    "        for i, (score, error) in enumerate(zip(scores, errors)):\n",
    "            plt.text(i, score + error + 0.01, f'{score:.4f} ± {error:.4f}', \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.ylabel('Accuracy Score')\n",
    "        plt.title(f'Forward Selection - {results[\"model_name\"]}\\n'\n",
    "                 f'Reducción: {X_original.shape[1]} → {results[\"n_features_selected\"]} características')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\" Comparación:\")\n",
    "        print(f\"      • Todas las características ({X_original.shape[1]}): {all_features_score.mean():.4f}\")\n",
    "        print(f\"      • Forward Selection ({results['n_features_selected']}): {results['mean_score']:.4f}\")\n",
    "        print(f\"      • Mejora: {results['mean_score'] - all_features_score.mean():+.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error al graficar {results['model_name']}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b94978",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "\n",
    "    resultados = run_forward_selection(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        X=X_scaled,  # Tu DataFrame de características\n",
    "        y=y,         # Tu variable objetivo\n",
    "        n_features='auto',  # Selecciona automáticamente la mitad\n",
    "        cv_folds=5\n",
    "    )\n",
    "        \n",
    "    # Generar gráfico comparativo\n",
    "    plot_forward_selection_results(resultados, X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e75b309",
   "metadata": {},
   "source": [
    "**Backward Elimination - Implementación Completa**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283108e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backward_elimination(model, model_name, X, y, n_features='auto', cv_folds=5):\n",
    "\n",
    "    print(f\"\\n{model_name} - Backward Elimination:\")\n",
    "    \n",
    "    try:\n",
    "        # Si es 'auto', selecciona la mitad de las características\n",
    "        if n_features == 'auto':\n",
    "            n_features = max(1, X.shape[1] // 2)\n",
    "        \n",
    "        # Configurar Backward Elimination\n",
    "        sfs_backward = SequentialFeatureSelector(\n",
    "            estimator=model,\n",
    "            n_features_to_select=n_features,\n",
    "            direction='backward',\n",
    "            cv=StratifiedKFold(cv_folds),\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Ajustar el selector\n",
    "        sfs_backward.fit(X, y)\n",
    "        \n",
    "        # Obtener características seleccionadas\n",
    "        selected_features = X.columns[sfs_backward.get_support()]\n",
    "        feature_mask = sfs_backward.get_support()\n",
    "        \n",
    "        # Evaluar performance con las características seleccionadas\n",
    "        X_selected = sfs_backward.transform(X)\n",
    "        cv_scores = cross_val_score(model, X_selected, y, cv=cv_folds, scoring='accuracy')\n",
    "        \n",
    "        # Recopilar resultados\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'method': 'Backward Elimination',\n",
    "            'n_features_selected': len(selected_features),\n",
    "            'selected_features': selected_features,\n",
    "            'feature_mask': feature_mask,\n",
    "            'mean_score': cv_scores.mean(),\n",
    "            'std_score': cv_scores.std(),\n",
    "            'cv_scores': cv_scores,\n",
    "            'sfs_object': sfs_backward,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        print(f\">>> Características seleccionadas: {results['n_features_selected']}\")\n",
    "        print(f\">>> Score de validación: {results['mean_score']:.4f} ± {results['std_score']:.4f}\")\n",
    "        print(f\">>> Características:\")\n",
    "        for i, feature in enumerate(results['selected_features'], 1):\n",
    "            print(f\"     {i}. {feature}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"   ❌ Error con {model_name}: {e}\"\n",
    "        print(error_msg)\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'method': 'Backward Elimination',\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def plot_backward_elimination_results(results, X_original):\n",
    "\n",
    "    if not results['success']:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Comparar performance vs todas las características\n",
    "        model = results['sfs_object'].estimator\n",
    "        all_features_score = cross_val_score(model, X_original, y, cv=5, scoring='accuracy')\n",
    "        \n",
    "        # Preparar datos para el gráfico\n",
    "        methods = ['Todas las características', 'Backward Elimination']\n",
    "        scores = [all_features_score.mean(), results['mean_score']]\n",
    "        errors = [all_features_score.std(), results['std_score']]\n",
    "        \n",
    "        # Crear gráfico de comparación\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(methods, scores, yerr=errors, capsize=10, \n",
    "                      color=['lightblue', 'lightcoral'], alpha=0.7)\n",
    "        \n",
    "        # Añadir valores en las barras\n",
    "        for i, (score, error) in enumerate(zip(scores, errors)):\n",
    "            plt.text(i, score + error + 0.01, f'{score:.4f} ± {error:.4f}', \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.ylabel('Accuracy Score')\n",
    "        plt.title(f'Backward Elimination - {results[\"model_name\"]}\\n'\n",
    "                 f'Reducción: {X_original.shape[1]} → {results[\"n_features_selected\"]} características')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\" Comparación:\")\n",
    "        print(f\"      • Todas las características ({X_original.shape[1]}): {all_features_score.mean():.4f}\")\n",
    "        print(f\"      • Backward Elimination ({results['n_features_selected']}): {results['mean_score']:.4f}\")\n",
    "        print(f\"      • Mejora: {results['mean_score'] - all_features_score.mean():+.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error al graficar {results['model_name']}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3452da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    resultados = run_backward_elimination(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        X=X,\n",
    "        y=y\n",
    "    )\n",
    "    \n",
    "    \n",
    "    plot_forward_selection_results(resultados, X)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "77695-data-science-i-flex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
